{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c7ec1d1",
   "metadata": {},
   "source": [
    "<img src=\"https://www.endocrine.org/-/media/endocrine/images/patient-engagement-webpage/condition-page-images/cardiovascular-disease/cardio_disease_t2d_pe_1796x943.jpg\" alt=\"Alternative text\" />\n",
    "\n",
    "# ‚ù§Ô∏è Topic: Cardio Vascular Disease (CVD)\n",
    "\n",
    "Dataset taken from: https://www.kaggle.com/datasets/sulianova/cardiovascular-disease-dataset\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ú® Introduction:\n",
    "### üí° What is Cardiovascular Disease?\n",
    "Cardiovascular disease (CVD) is a general term that describes a disease of the heart or blood vessels. The term refers to set of diseases that one can be diagnosed with when blood flow to the heart, brain or body is reduced due to blood clots (Thrombosis) or a build-up of fatty deposits inside an artery, which usually leads to the artery becoming hard and narrow.\n",
    "\n",
    "### 4Ô∏è‚É£ Main Types of CVD:\n",
    "1. Coronary Heart Disease\n",
    "2. Stroke\n",
    "3. Peripheral Arterial Disease\n",
    "4. Aortic Disease\n",
    "\n",
    "### üìà Causing Factors:\n",
    "\n",
    "Risk factors for CVD include smoking, high blood pressure, high cholesterol levels, obesity, diabetes, and a family history of the disease. Lifestyle changes, such as maintaining a healthy diet and exercise regimen, quitting smoking, and managing these risk factors, can help prevent CVD. Additionally, medications and medical procedures may be recommended to treat or prevent CVD in certain cases.\n",
    "\n",
    "### üìä Dataset:\n",
    "The Dataset that we have chosen is taken from Kaggle and contains data of a group of 70,000 patients, with CVD present in some of them. The data depicts certain factors and characteristics of these patients, some of which may have contributed to their CVD. \n",
    "\n",
    "We will explore this Dataset in this project.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùóProblem Description:\n",
    "\n",
    "Our aim of this project is to use different Machine Learning Models and Algorithms to analyse the cardio-vascular dataset and the accuracy of the various models used and see which is the most accurate and suitable for predicting the presence of cardio-vascular diseases in a person.\n",
    "\n",
    "We will do this by passing the cardio vascular dataset we have obtained from kaggle through the Machine Learning Models and also performing different Exploratory Data Analysis techniques on the dataset to prepare the dataset fully before implementation.\n",
    "\n",
    "---\n",
    "## üìñ Table of Contents:\n",
    "\n",
    "### ‚¨áÔ∏è 1.0 Importing Data\n",
    "1.1 Importing Essential Libraries and Dataset   \n",
    "1.2 Importing Essential Functions   \n",
    "1.3 Describing the Variables\n",
    "\n",
    "---\n",
    "\n",
    "### üîé 2.0 Variable Analysis\n",
    "2.1 Numerical Variable Analysis   \n",
    "2.2 Categorical Variable Analysis\n",
    "\n",
    "---\n",
    "\n",
    "### üó∫Ô∏è 3.0 Exploratory Data Analysis:\n",
    "3.1 Cleaning of Data and Removal of Outliers   \n",
    "3.2 Cleaned Variable Conclusion\n",
    "\n",
    "---\n",
    "\n",
    "### ü§ñ 4.0 Machine Learning Models:\n",
    "4.1 XGBoost    \n",
    "4.2 XGBoost Analysis and Conclusion    \n",
    "4.3 Logistic Regression    \n",
    "4.4 Logistic Regression Analysis and Conclusion  \n",
    "\n",
    "---\n",
    "\n",
    "### üñäÔ∏è 5.0 Conclusion:\n",
    "5.1 Comparing XGBoost and Logistic Regression    \n",
    "5.2 Epilogue and Conclusion    \n",
    "5.3 Models Used    \n",
    "5.4 What did we learn from this project?    \n",
    "5.5 References    \n",
    "\n",
    "---\n",
    "## ‚¨áÔ∏è1.0 Importing Essential Libraries & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad512823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Propose\n",
    "# import pickle\n",
    "# import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "# from scipy.stats import shapiro\n",
    "# from scikitplot import metrics as mt\n",
    "\n",
    "# Hipo Test\n",
    "# from scipy import stats\n",
    "# from scipy.stats import f_oneway\n",
    "# from scipy.stats import ttest_ind\n",
    "\n",
    "\n",
    "# Pre-processing\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.feature_selection import VarianceThreshold\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder\n",
    "\n",
    "# Modelling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import classification_report, accuracy_score, cohen_kappa_score, precision_score, f1_score, recall_score\n",
    "# from yellowbrick.classifier.threshold import discrimination_threshold\n",
    "\n",
    "df = pd.read_excel(\"cardio_train.xlsx\")\n",
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e5cdec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7db19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43200c0f",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.1 Importing essential functions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1db58df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    q1 = df.quantile(0.25)\n",
    "    q3 = df.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    df_out = df[~((df < (q1 - 1.5 * iqr)) | (df > (q3 + 1.5 * iqr))).any(axis=1)]\n",
    "    count = ( ((df < (q1 - 1.5 * iqr)) | (df > (q3 + 1.5 * iqr) ))).sum()\n",
    "    print(count)\n",
    "    return df_out\n",
    "    \n",
    "\n",
    "# Helper Functions\n",
    "def balanced_target(target, dataset, hue=None):\n",
    "    \"\"\"\n",
    "    Function to check the balancing of the target variable.\n",
    "\n",
    "    :target:  An pd.Series of the target variable that will be checked.\n",
    "    :dataset: An Dataframe object. \n",
    "    \"\"\"\n",
    "    sb.set(style='darkgrid', palette='Accent')\n",
    "    ax = sb.countplot(x=target, hue=hue, data=dataset)\n",
    "    ax.figure.set_size_inches(10, 6)\n",
    "    ax.set_title('Cardio Distribution', fontsize=18, loc='left')\n",
    "    ax.set_xlabel(target, fontsize=14)\n",
    "    ax.set_ylabel('Count', fontsize=14)\n",
    "    ax=ax\n",
    "\n",
    "\n",
    "def univariate_analysis(target, df):\n",
    "    \"\"\"\n",
    "    Function to perform univariate analysis.\n",
    "\n",
    "    df: DataFrame\n",
    "    \"\"\"\n",
    "    for col in df.columns.to_list():\n",
    "\n",
    "        fig = sb.displot(x=col, hue=target, data=df, kind='hist')\n",
    "        fig.set_titles(f'{col}\\n distribuition', fontsize=16)\n",
    "        fig.set_axis_labels(col, fontsize=14)\n",
    "\n",
    "\n",
    "def multi_histogram(data: pd.DataFrame, variables: list) -> None:\n",
    "\n",
    "    # set of initial plot posistion\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    n = 1\n",
    "    for column in data[variables].columns:\n",
    "        plt.subplot(3, 3, n)\n",
    "        _ = sb.distplot(a=data[column], bins=50, hist=True)\n",
    "        n += 1\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.3)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def multi_boxplot(data: pd.DataFrame, variables: list) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Function to check for outliers visually through a boxplot\n",
    "\n",
    "    data: DataFrame\n",
    "\n",
    "    variable: list of numerical variables\n",
    "    \"\"\"\n",
    "\n",
    "    # set of initial plot posistion\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    n = 1\n",
    "    for column in data[variables].columns:\n",
    "        plt.subplot(3, 3, n)\n",
    "        _ = sb.boxplot(x=column, data=data)\n",
    "        n += 1\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.3)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def hipo_test(*samples):\n",
    "\n",
    "    samples = samples\n",
    "\n",
    "    try:\n",
    "        if len(samples) == 2:\n",
    "            stat, p = ttest_ind(*samples)\n",
    "        elif len(samples) > 2:\n",
    "            stat, p = f_oneway(*samples)\n",
    "    except:\n",
    "        raise Exception(\"Deve ser fornecido pelo menos duas samples!!!\")\n",
    "\n",
    "    if p < 0.05:\n",
    "        print(f'O valor de p √©: {p}')\n",
    "        print('Prov√°vel haver diferen√ßa')\n",
    "    else:\n",
    "        print(f'O valor de p √©: {p}')\n",
    "        print('Prov√°vel que n√£o haja diferen√ßa')\n",
    "\n",
    "    return stat, p\n",
    "\n",
    "\n",
    "def point_bi_corr(a, b):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to calculate point biserial correlation coefficient heatmap function\n",
    "    Credits: Bruno Santos - Comunidade DS\n",
    "\n",
    "    :a: input dataframe with binary variable\n",
    "    :b: input dataframe with continous variable\n",
    "    \"\"\"\n",
    "\n",
    "    # Get column name\n",
    "    a = a.values.reshape(-1)\n",
    "    b = b.columns.reshape(-1)\n",
    "\n",
    "    # apply scipys point-biserial\n",
    "    stats.pointbiserialr(a, b)\n",
    "\n",
    "    # correlation coefficient array\n",
    "    c = np.corrcoef(a, b)\n",
    "\n",
    "    # dataframe for heatmap\n",
    "    df = pd.DataFrame(c, columns=[a, b], index=[a, b])\n",
    "\n",
    "    # return heatmap\n",
    "    return sb.heatmap(df, annot=True).set_title('{} x {} correlation heatmap'.format(a, b));\n",
    "\n",
    "\n",
    "def change_threshold_lgbm(X, y, model, n_splits, thresh):\n",
    "\n",
    "    # cross-valida√ß√£o\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
    "\n",
    "    acc = []\n",
    "    kappa = []\n",
    "    recall = []\n",
    "    for linhas_treino, linhas_valid in skf.split(X, y):\n",
    "\n",
    "        X_treino, X_valid = X.iloc[linhas_treino], X.iloc[linhas_valid]\n",
    "        y_treino, y_valid = y.iloc[linhas_treino], y.iloc[linhas_valid]\n",
    "\n",
    "        pred_prob = model.predict_proba(X_valid)\n",
    "\n",
    "        for i in range(0, len(pred_prob)):\n",
    "            if pred_prob[i, 1] >= thresh:\n",
    "                pred_prob[i, 1] = 1\n",
    "            else:\n",
    "                pred_prob[i, 1] = 0\n",
    "\n",
    "        Acc = accuracy_score(y_valid, pred_prob[:, 1])\n",
    "        Kappa =  cohen_kappa_score(y_valid, pred_prob[:, 1])\n",
    "        Recall = recall_score(y_valid, pred_prob[:, 1])\n",
    "        acc.append(Acc)\n",
    "        kappa.append(Kappa)\n",
    "        recall.append(Recall)\n",
    "\n",
    "    print('####### Business Metrics #######')\n",
    "    print('\\n')\n",
    "    acc_inc = np.mean(acc) - 0.50\n",
    "    prc_inc = round((acc_inc/0.05)*500, 2)\n",
    "    print(f'Increased precision: {round(acc_inc,2)}')\n",
    "    print(f'Price Increased in: {prc_inc}')\n",
    "    print(f'Percentual of Price increassing: {round(prc_inc/500,2)}')\n",
    "    print('\\n')\n",
    "\n",
    "    # print classification report\n",
    "    print('####### Machine Learning Metrics #######\\n')\n",
    "    print(classification_report(y_valid, pred_prob[:,1], digits=2))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    mt.plot_confusion_matrix(y_valid, pred_prob[:,1], normalize=False, figsize=(10,8))\n",
    "\n",
    "    return pred_prob[:, 1]\n",
    "\n",
    "\n",
    "def change_threshold_lr(X, y, model, n_splits, thresh):\n",
    "    # cross-valida√ß√£o\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
    "\n",
    "    acc = []\n",
    "    kappa = []\n",
    "    recall = []\n",
    "    for linhas_treino, linhas_valid in skf.split(X, y):\n",
    "\n",
    "        X_treino, X_valid = X.iloc[linhas_treino], X.iloc[linhas_valid]\n",
    "        y_treino, y_valid = y.iloc[linhas_treino], y.iloc[linhas_valid]\n",
    "\n",
    "        y_scores_final = model.decision_function(X_valid)\n",
    "        y_pred_recall = (y_scores_final > thresh)\n",
    "\n",
    "        Acc = accuracy_score(y_valid, y_pred_recall)\n",
    "        Kappa =  cohen_kappa_score(y_valid, y_pred_recall)\n",
    "        Recall = recall_score(y_valid, y_pred_recall)\n",
    "        acc.append(Acc)\n",
    "        kappa.append(Kappa)\n",
    "        recall.append(Recall)\n",
    "\n",
    "    print('####### Bussines Metrics #######\\n')\n",
    "\n",
    "    acc_inc = np.mean(acc) - 0.50\n",
    "    prc_inc = round((acc_inc/0.05)*500, 2)\n",
    "    print(f'Increased precision: {round(acc_inc,2)}')\n",
    "    print(f'Price Increased in: {prc_inc}')\n",
    "    print(f'Percentual of Price increassing: {round(prc_inc/500,2)}')\n",
    "    print('\\n')\n",
    "\n",
    "    print('####### Machine Learning Metrics #######\\n')\n",
    "    print(f'New kappa: {cohen_kappa_score(y_valid,y_pred_recall)}\\n')\n",
    "    print(classification_report(y_valid, y_pred_recall, digits=2))\n",
    "\n",
    "\n",
    "    return y_pred_recall\n",
    "\n",
    "\n",
    "################################################# Custons Transformers ###########################################################\n",
    "\n",
    "class PreProcessingTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        Xtemp = X.copy()\n",
    "\n",
    "        # Height\n",
    "        index_height = Xtemp.loc[Xtemp['height'] > 230, ['height']].index\n",
    "        Xtemp.drop(index_height, inplace=True)\n",
    "        index_height1 = Xtemp.loc[Xtemp['height'] < 112, ['height']].index\n",
    "        Xtemp.drop(index_height1, inplace=True)\n",
    "\n",
    "        # Weight\n",
    "        index_weight = Xtemp.loc[Xtemp['weight'] < 40, ['weight']].index\n",
    "        Xtemp.drop(index_weight, inplace=True)\n",
    "\n",
    "        # ap_hi\n",
    "        index_ap_hi = Xtemp.loc[Xtemp['ap_hi'] < 10, ['ap_hi']].index\n",
    "        Xtemp.drop(index_ap_hi, inplace=True)\n",
    "\n",
    "        # ap_lo\n",
    "        index_ap_lo = Xtemp.loc[Xtemp['ap_lo'] < 5, ['ap_lo']].index\n",
    "        Xtemp.drop(index_ap_lo, inplace=True)\n",
    "\n",
    "        # SMOTE + TOMEKLINK\n",
    "        X = Xtemp.drop('cardio', axis=1)\n",
    "        y = Xtemp['cardio']\n",
    "\n",
    "        smt = SMOTETomek(random_state=42)\n",
    "        Xres, yres = smt.fit_resample(X, y)\n",
    "        Xtemp = pd.concat([Xres, yres], axis=1)\n",
    "\n",
    "        return Xtemp\n",
    "\n",
    "\n",
    "class FeatureEngineeringTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        Xtemp = X.copy()\n",
    "\n",
    "        # Cluster based var\n",
    "        kmeans = KMeans(n_clusters=2, init='k-means++',n_init=20, random_state=0).fit(Xtemp)\n",
    "        Xtemp['kmeans_cat'] = kmeans.labels_\n",
    "\n",
    "        # # Cluster GMM\n",
    "        # gmm = GaussianMixture(n_components=3).fit(Xtemp)\n",
    "        # Xtemp['gauss_cat'] = gmm.predict(Xtemp)\n",
    "\n",
    "        # Year_age\n",
    "        Xtemp['year_age'] = Xtemp['age'] / 365\n",
    "\n",
    "        # drop 'id' and 'age' 'smoke','alco','gluc', 'ap_lo', 'cholesterol', 'height', 'active', 'weight'\n",
    "        Xtemp.drop(['id', 'age'], inplace=True, axis=1)\n",
    "\n",
    "        # IMC\n",
    "        Xtemp['imc'] = Xtemp['weight']/(Xtemp['height']/100)**2\n",
    "\n",
    "        # cat_dwarfism\n",
    "        Xtemp['cat_Dwarfism'] = [1 if value < 145 else 0 for value in Xtemp['height']]\n",
    "\n",
    "        # ap_hi divide 10\n",
    "        Xtemp.loc[Xtemp['ap_hi'] > 220, ['ap_hi']] = Xtemp.loc[Xtemp['ap_hi'] > 220, ['ap_hi']]/10\n",
    "\n",
    "        # ap_lo divide 10\n",
    "        Xtemp.loc[Xtemp['ap_lo'] > 190, ['ap_lo']] = Xtemp.loc[Xtemp['ap_lo'] > 190, ['ap_lo']]/10\n",
    "\n",
    "        # ap_hi divide 10\n",
    "        Xtemp.loc[Xtemp['ap_hi'] > 220, ['ap_hi']] = Xtemp.loc[Xtemp['ap_hi'] > 220,['ap_hi']]/10\n",
    "\n",
    "        # ap_hi divide 10\n",
    "        Xtemp.loc[Xtemp['ap_hi'] > 220, ['ap_hi']] = Xtemp.loc[Xtemp['ap_hi'] > 220,['ap_hi']]/10\n",
    "\n",
    "        # ap_lo divide 10\n",
    "        Xtemp.loc[Xtemp['ap_lo'] > 190, ['ap_lo']] = Xtemp.loc[Xtemp['ap_lo'] > 190,['ap_lo']]/10\n",
    "\n",
    "        return Xtemp\n",
    "\n",
    "\n",
    "class CatBloodPressureTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        Xtemp = X.copy()\n",
    "\n",
    "        # cat_bloodpressure\n",
    "        def cat_bloodpressure(df):\n",
    "\n",
    "            if df['ap_hi'] < 90 and df['ap_lo'] < 60:\n",
    "                return 1 #Hipotens√£o\n",
    "            elif 90 <= df['ap_hi'] < 140 and 60 <= df['ap_lo'] < 90:\n",
    "                return 2    # Pr√©-Hipotens√£o\n",
    "            elif 140 <= df['ap_hi'] < 160 and 90 <= df['ap_lo'] < 100:\n",
    "                return 3  # 'Hipertens√£o estagio1'\n",
    "            elif df['ap_hi'] >= 160 and df['ap_lo'] >= 100:\n",
    "                return 4 # 'Hipertens√£o estagio2'\n",
    "            else:\n",
    "                return 5 # 'no_cat'\n",
    "\n",
    "        # cat_bloodpressure\n",
    "        Xtemp['cat_bloodpressure'] = Xtemp.apply(cat_bloodpressure, axis=1)\n",
    "\n",
    "        return Xtemp\n",
    "\n",
    "\n",
    "class TotalPressureTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "\n",
    "        Xtemp = X.copy()\n",
    "\n",
    "        # total_preassure\n",
    "        Xtemp['total_pressure'] = Xtemp['ap_hi'] + Xtemp['ap_lo']\n",
    "\n",
    "        return Xtemp\n",
    "\n",
    "\n",
    "class MyRobustScalerTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "\n",
    "        Xtemp = X.copy()\n",
    "\n",
    "        scaler = RobustScaler()\n",
    "        Xscaled = scaler.fit_transform(Xtemp)\n",
    "        Xtemp = pd.DataFrame(Xscaled, columns=Xtemp.columns.to_list())\n",
    "\n",
    "        return Xtemp\n",
    "\n",
    "    \n",
    "# XGBoost Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1f63c9",
   "metadata": {},
   "source": [
    "## 1.2 Describing the Variables:\n",
    "---\n",
    "\n",
    "### Different Variables:\n",
    "\n",
    "    1. Age - Numerical - Days\n",
    "    2. Height - Numerical - Cm\n",
    "    3. Weight - Numerical - Kg\n",
    "    4. Gender - Categorical - 1/0\n",
    "    5. Systolic Blood Pressure (ap_hi) - Numerical - mmHg\n",
    "    6. Systolic Blood Pressure (ap_lo) - Numerical - mmHg\n",
    "    7. Cholesterol - Categorical - 1-3\n",
    "    8. Glucose - Categorical - 1-3\n",
    "    9. Smoke - Categorical - 1/0\n",
    "    10. Alcohol Intake - Categorical - 1/0\n",
    "    11. Physical Activity - Categorical - 1/0\n",
    "    12. Cardio - Categorical - 1/0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb5d000",
   "metadata": {},
   "source": [
    "# üîé 2.0 Variable Analysis\n",
    "---\n",
    "\n",
    "First we will explore the Numerical Variables, then the Categorical Variables.\n",
    "\n",
    "## 2.1 Numerical Variable Analysis\n",
    "\n",
    "For Numerical Variables, we will use a boxplot and plot them against cardio (i.e the presence of cardio vascular disease)\n",
    "\n",
    "Afterwhich, we will plot a histogram for each of the variables to check their skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b58232",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df[['ap_hi', 'cardio']].copy()\n",
    "\n",
    "df_new = remove_outliers(df_new)\n",
    "f = plt.figure(figsize=(20,10))\n",
    "sb.boxplot(data=df_new, orient='v',x='cardio',y='ap_hi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61ae1b0",
   "metadata": {},
   "source": [
    "Based on the American Heart Association guidelines for blood pressure , anything outside the range of for Systolic BP (ap_hi) from 50 to 200 is abnormal. Hence, we will be removing them from the api_hi column. For Diastolic BP (ap_lo) anything outside the range from 60 to 90 is abnormal, but the extreme and uncommon cases are those that are below 50 and above 120. Hence, we will be removing them from the dataset.\n",
    "\n",
    "- Taken from https://www.heart.org/en/health-topics/high-blood-pressure/understanding-blood-pressure-readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15caa1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the Outliers based on the above description\n",
    "df = df.copy()\n",
    "index = df.loc[(df['ap_hi']<50)|(df['ap_hi']>200),['ap_hi']].index\n",
    "df.drop(index, inplace=True)\n",
    "index = df.loc[(df['ap_lo']<50)|(df['ap_lo']>120),['ap_lo']].index\n",
    "df.drop(index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97af3ac9",
   "metadata": {},
   "source": [
    "### Plotting the Boxplots and Histograms for the Numerical Variables.\n",
    "\n",
    "Boxplots are to see the spread of the graphs and identify any significant outliers while Histograms are to see the general skews of the graphs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d6923b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(20,10))\n",
    "sb.boxplot(data=df, orient='v',x='cardio',y='age')\n",
    "f = plt.figure(figsize=(20,10))\n",
    "sb.boxplot(data=df, orient='v',x='cardio',y='height')\n",
    "f = plt.figure(figsize=(20,10))\n",
    "sb.boxplot(data=df, orient='v',x='cardio',y='weight')\n",
    "f = plt.figure(figsize=(20,10))\n",
    "sb.boxplot(data=df, orient='v',x='cardio',y='ap_hi')\n",
    "f = plt.figure(figsize=(20,10))\n",
    "sb.boxplot(data=df, orient='v',x='cardio',y='ap_lo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee5775",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(16, 8))\n",
    "sb.histplot(data = df[\"age\"], kde=True)\n",
    "f = plt.figure(figsize=(16, 8))\n",
    "sb.histplot(data = df[\"height\"], kde=True)\n",
    "f = plt.figure(figsize=(16, 8))\n",
    "sb.histplot(data = df[\"weight\"], kde=True)\n",
    "f = plt.figure(figsize=(16, 8))\n",
    "sb.histplot(data = df[\"ap_hi\"], kde=True)\n",
    "f = plt.figure(figsize=(16, 8))\n",
    "sb.histplot(data = df[\"ap_lo\"], kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f2faed",
   "metadata": {},
   "source": [
    "Looking at the ap_hi and ap_lo histograms we can infer that the distribution very unevenly distributed, hence we will not be using these columns in our model prediction.\n",
    "\n",
    "Looking at the height and weight plots, it seems that BMI is a better indicator as it takes into consideration both the height and weight to predict cardio. \n",
    "\n",
    "Hence, we will create a BMI column using the formula: weight/(height^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bf7056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of BMI Column using weight and height(m) \n",
    "df['BMI'] = df['weight'] / ((df['height'])/100)**2\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd81983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the box plots for BMI\n",
    "f = plt.figure(figsize=(20,10))\n",
    "sb.boxplot(data=df, orient='v',x='cardio',y='BMI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ea0f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BMI # > 130: \" , df.loc[df['BMI']>130,:].size)\n",
    "print(\"BMI # < 10: \" , df.loc[df['BMI']<10,:].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0a38b9",
   "metadata": {},
   "source": [
    "Since it is extremely rare for anyone's BMI to be above 130 and below 10, there may have been errors in the data. Hence, we will remove them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ad4cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "index = df.loc[(df['BMI']<10)|(df['BMI']>130),['BMI']].index\n",
    "df.drop(index, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23fce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['age','height','weight','ap_hi','ap_lo']].skew(axis=0,skipna=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c638e4",
   "metadata": {},
   "source": [
    "From the data, ap_hi is the most skewed from a regular normal distribution. With a high positive skew of 85.296214.\n",
    "\n",
    "---\n",
    "\n",
    "### Plotting Correlation & Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb0b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "numvars=df[['age','height','weight','ap_hi','ap_lo', 'BMI']]\n",
    "numvars.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab06d44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.heatmap(numvars.corr(),vmin=-1,vmax=1,annot=True,fmt=\".4f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09555053",
   "metadata": {},
   "source": [
    "As seen in the heatmap above, the height and weight has a strong correlation with the newly created BMI column, hence we will be using the BMI column instead of the height and weight columns to train our models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db265753",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.2 Categorical Variable Analysis:\n",
    "\n",
    "For Categorical Variables, we will use a heatmap and correlation matrix.\n",
    "\n",
    "We will first put these categorical Variables into one seperate DataFrame so as to more easily compare and contrast with 'cardio'.\n",
    "\n",
    "The 6 Categorical Variables we are exploring are:\n",
    "1. Gender\n",
    "2. Cholesterol\n",
    "3. Glucose\n",
    "4. Smoking\n",
    "5. Alcohol Intake\n",
    "6. Physical Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b40a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cardio distribution by 'cholesterol' (Cholesterol)\n",
    "balanced_target(target='cholesterol', hue='cardio', dataset=df)\n",
    "\n",
    "total_cardio_0 = df['cardio'].value_counts()[0]\n",
    "total_cardio_1 = df['cardio'].value_counts()[1]\n",
    "\n",
    "print('Total count of cardio = 0:', total_cardio_0)\n",
    "print('Total count of cardio = 0:', total_cardio_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045ecc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cardio distribution by 'gluc' (Glucose)\n",
    "balanced_target(target='gluc',hue='cardio', dataset=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e2858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cardio distribution by 'alco' (Alcohol)\n",
    "balanced_target(target='alco',hue='cardio', dataset=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c2df3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cardio distribution by 'active' (Physical Activity)\n",
    "balanced_target(target='active',hue='cardio', dataset=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1661a3",
   "metadata": {},
   "source": [
    "# üó∫Ô∏è 3.0 Exploratory Data Analysis:\n",
    "---\n",
    "\n",
    "## 3.1 Cleaning of Data and Removal of Outliers\n",
    "\n",
    "In this section, we will change our categorical variables into numerical that will be represented by values 0,1,2.. \n",
    "\n",
    "This allows us to better and more easily use the dataset for the upcoming Machine Learning Models that will implement in the next sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d69d98",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Changing column names and variable description to make the data more readable\n",
    "df_cleaned = df.copy()\n",
    "df_cleaned.rename(columns = {'ap_hi': 'Systolic_BP'}, inplace = True)\n",
    "df_cleaned.rename(columns = {'ap_lo': 'Diastolic_BP'}, inplace = True)\n",
    "df_cleaned.rename(columns = {'gluc': 'Glucose'}, inplace = True)\n",
    "df_cleaned.rename(columns = {'alco': 'Alcohol'}, inplace = True)\n",
    "df_cleaned.rename(columns = {'cardio': 'Cardio_Patient'}, inplace = True)\n",
    "\n",
    "# Dropping unneeded ID column\n",
    "df_cleaned.drop(\"id\", axis=1, inplace=True);\n",
    "\n",
    "# Replacing data in columns for readibility\n",
    "df_cleaned['gender'] = df_cleaned['gender'].replace(1,0) # 0 is Male\n",
    "df_cleaned['gender'] = df_cleaned['gender'].replace(2, 1) # 1 is Female\n",
    "# df_cleaned['cholesterol'] = df_cleaned['cholesterol'].replace(1, 'Normal')\n",
    "# df_cleaned['cholesterol'] = df_cleaned['cholesterol'].replace(2, 'Above_Normal')\n",
    "# df_cleaned['cholesterol'] = df_cleaned['cholesterol'].replace(3, 'Well_Above_Normal')\n",
    "# df_cleaned['Glucose'] = df_cleaned['Glucose'].replace(1, 'Normal')\n",
    "# df_cleaned['Glucose'] = df_cleaned['Glucose'].replace(2, 'Above_Normal')\n",
    "# df_cleaned['Glucose'] = df_cleaned['Glucose'].replace(3, 'Well_Above_Normal')\n",
    "# df_cleaned['smoke'] = df_cleaned['smoke'].replace(1, 'Yes')\n",
    "# df_cleaned['smoke'] = df_cleaned['smoke'].replace(0, 'No')\n",
    "# df_cleaned['Alcohol'] = df_cleaned['Alcohol'].replace(1, 'Yes')\n",
    "# df_cleaned['Alcohol'] = df_cleaned['Alcohol'].replace(0, 'No')\n",
    "# df_cleaned['active'] = df_cleaned['active'].replace(1, 'Yes')\n",
    "# df_cleaned['active'] = df_cleaned['active'].replace(0, 'No')\n",
    "# df_cleaned['Cardio_Patient'] = df_cleaned['Cardio_Patient'].replace(1, 'Yes')\n",
    "# df_cleaned['Cardio_Patient'] = df_cleaned['Cardio_Patient'].replace(0, 'No')\n",
    "\n",
    "# Set all columns to upper case\n",
    "df_cleaned.columns = df_cleaned.columns.str.upper()\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ac39e5",
   "metadata": {},
   "source": [
    "## 3.2 Cleaned Variable Conclusion:\n",
    "\n",
    "Format: Column Description (COLUMN NAME):\n",
    "\n",
    "    1. Age (AGE) - Numerical - Days\n",
    "    2. Height (HEIGHT) - Numerical - Cm\n",
    "    3. Weight (WEIGHT) - Numerical - Kg\n",
    "    4. Gender (GENDER) - Categorical - 0 (Male), 1 (Female)\n",
    "    5. Systolic Blood Pressure (SYSTOLIC_BP) - Numerical - mmHg\n",
    "    6. Diastolic Blood Pressure (DIASTOLIC_BP) - Numerical - mmHg\n",
    "    7. Cholesterol (CHOLESTEROL) - Categorical - 1 (Normal), 2 (Above Normal), 3 (Well Above Normal)\n",
    "    8. Glucose (GLOCOSE) - Categorical - 1 (Normal), 2 (Above Normal), 3 (Well Above Normal) \n",
    "    9. Smoking (SMOKE) - Categorical - 1 (Yes), 0 (No)\n",
    "    10. Alcohol Intake (ALCOHOL) - Categorical - 1 (Yes), 0 (No)\n",
    "    11. Physical Activity (ACTIVE) - Categorical - 1 (Yes), 0 (No)\n",
    "    12. Patient of Cardio Disease (CARDIO_PATIENT) - Categorical - 1 (Yes), 0 (No)\n",
    "    13. BMI - Numerical - Kg/m**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858eb74e",
   "metadata": {},
   "source": [
    "Based on the information provided, a suitable reason to justify the exclusion of heart rate from the variables used in the cardiovascular disease prediction model is that heart rate may not be a direct or accurate measure of cardiovascular disease risk. Although heart rate is commonly used as an indicator of overall cardiovascular health, there are many other factors that can affect heart rate, such as stress, physical activity, and even medication. Additionally, heart rate is not always a reliable indicator of heart disease risk, as some individuals with a normal heart rate may still be at increased risk for cardiovascular events.\n",
    "\n",
    "Therefore, to build a more accurate and reliable cardiovascular disease prediction model, it may be best to focus on the other variables mentioned, such as age, BMI, cholesterol, glucose, smoking, and alcohol consumption. These factors have been shown to be more directly linked to cardiovascular disease risk and can provide a more comprehensive assessment of an individual's overall health and risk for developing cardiovascular disease.\n",
    "\n",
    "In conclusion, the exclusion of heart rate from the cardiovascular disease prediction model is justifiable based on its limited direct correlation with cardiovascular disease risk. By focusing on the other variables that have been shown to be more closely linked to cardiovascular disease risk, we can build a more accurate and reliable model for predicting an individual's risk of developing this condition.\n",
    "\n",
    "---\n",
    "# ü§ñ 4.0 Machine Learning Models:\n",
    "---\n",
    "## 4.1 XGBoost\n",
    "\n",
    "#### What is XGBoost?\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) is an open-source machine learning library that implements gradient boosting algorithms. It was developed by Tianqi Chen and Carlos Guestrin in 2016 and has become one of the most popular machine-learning libraries, winning several Kaggle competitions.\n",
    "\n",
    "Gradient boosting is a technique that combines multiple weak models to form a strong model. It does this by iteratively adding new models to the ensemble, with each model trying to correct the errors of the previous models. XGBoost is an implementation of gradient boosting that uses a tree-based model.\n",
    "\n",
    "XGBoost can handle both regression and classification problems and is known for its speed and accuracy. It uses a number of techniques to reduce overfitting, including regularization and early stopping. It also supports parallel processing, making it possible to train models on large datasets in a reasonable amount of time.\n",
    "\n",
    "Overall, XGBoost is a powerful machine-learning library widely used in industry and academia for various applications, including data mining, natural language processing, and computer vision.\n",
    "\n",
    "---\n",
    "#### What is Overfitting?\n",
    "\n",
    "Overfitting is a phenomenon that occurs when a machine learning model is trained too well on the training data, to the point that it begins to memorize it instead of generalizing the patterns. This means that the model has learned the training data so well that it does not perform well on new, unseen data.\n",
    "\n",
    "Overfitting occurs when the model is too complex, and it has too many parameters relative to the amount of training data. As a result, the model becomes too specialized to the training data and fails to capture the underlying patterns in the data that generalize well to new data.\n",
    "\n",
    "Overfitted models tend to have good performance with the data used to fit them (the training data), but they behave poorly with unseen data (or test data, which is data not used to fit the model).\n",
    "\n",
    "Overfitting can be detected by comparing the performance of the model on the training data versus the performance on the testing data. If the performance on the training data is significantly better than on the testing data, it is a sign of overfitting. To prevent overfitting, various techniques can be used, including regularization, cross-validation, and early stopping. Regularization adds a penalty term to the loss function to discourage the model from overfitting, cross-validation evaluates the performance of the model on different subsets of the data, and early stopping stops the training of the model when the performance on the testing data starts to deteriorate.\n",
    "\n",
    "---\n",
    "#### What is Regularization?\n",
    "\n",
    "Regularization is a technique used in machine learning to prevent the overfitting of the model to the training data. It involves adding a penalty term to the loss function that the model is optimizing during training. The penalty term discourages the model from fitting the training data too well and instead encourages it to find a more generalizable solution that can perform well on new, unseen data.\n",
    "\n",
    "There are two common types of regularization techniques used in machine learning:\n",
    "\n",
    "L1 Regularization (Lasso Regression): This technique adds a penalty term to the loss function that is proportional to the absolute value of the weights of the model. This penalty term shrinks the weights of the model towards zero, effectively removing some of the less important features from the model. This makes the model simpler and less likely to overfit.\n",
    "\n",
    "L2 Regularization (Ridge Regression): This technique adds a penalty term to the loss function that is proportional to the square of the weights of the model. This penalty term shrinks the weights of the model towards zero as well, but unlike L1 regularization, it does not remove any features completely from the model. Instead, it reduces the impact of less important features, making the model more robust to noise in the data.\n",
    "\n",
    "In summary, regularization is a powerful technique to combat overfitting in machine learning models. By adding a penalty term to the loss function, it encourages the model to find a simpler solution that generalizes well to new data.\n",
    "\n",
    "The XGBoost libraries support both L1 and L2 regularization. In XGBoost, L1 regularization is called \"L1 regularization\" or \"Lasso regularization,\" while L2 regularization is called \"L2 regularization\" or \"Ridge regularization.\"\n",
    "\n",
    "XGBoost allows users to specify the regularization type and the strength of regularization using the \"alpha\" and \"lambda\" hyperparameters. The \"alpha\" hyperparameter controls the L1 regularization strength, while the \"lambda\" hyperparameter controls the L2 regularization strength. By tuning these hyperparameters, users can adjust the amount of regularization applied to the model and prevent overfitting.\n",
    "\n",
    "In practice, a combination of L1 and L2 regularization is often used in XGBoost to achieve better performance. This is known as \"Elastic Net\" regularization, which combines the benefits of both L1 and L2 regularization. The Elastic Net regularization is controlled by two hyperparameters, alpha and lambda, which control the strength of the L1 and L2 regularization, respectively. By tuning these hyperparameters, users can balance the contribution of L1 and L2 regularization and achieve the best performance on their data.\n",
    "\n",
    "---\n",
    "#### What is Early Stopping?\n",
    "\n",
    "Early stopping is a technique used in machine learning to prevent overfitting of the model to the training data. It involves monitoring the performance of the model on a validation set during training and stopping the training process when the performance of the model on the validation set starts to deteriorate.\n",
    "\n",
    "During training, the model is evaluated on both the training set and the validation set at regular intervals (usually after each epoch). The training process is stopped when the performance of the model on the validation set starts to worsen. This is because continuing to train the model after this point will result in overfitting as the model begins to memorize the training data.\n",
    "\n",
    "The point at which the training is stopped is determined using a stopping criterion. One common stopping criterion is to stop training when the validation loss (i.e., the loss on the validation set) stops decreasing for a certain number of epochs. Another common stopping criterion is to stop training when the difference between the training loss and validation loss exceeds a certain threshold.\n",
    "\n",
    "Early stopping is a simple yet effective technique for preventing overfitting in machine learning models. By stopping the training process at the right time, it helps the model generalize better to new, unseen data and improves the overall performance of the model.\n",
    "\n",
    "---\n",
    "#### What is a hyperparameter?\n",
    "\n",
    "In machine learning, a hyperparameter is a parameter that is set before the model is trained and remains fixed throughout the training process. Unlike model parameters, which are learned from the training data during the training process, hyperparameters are set by the data scientist or machine learning engineer before training begins.\n",
    "\n",
    "Examples of hyperparameters include the learning rate of the model, the regularization strength, the number of hidden layers in a neural network, the number of trees in a random forest, and so on. These hyperparameters are set before training begins and are typically tuned through a process called hyperparameter tuning or hyperparameter optimization, where different values of the hyperparameters are tried to find the best combination that maximizes the performance of the model on a validation set or through cross-validation.\n",
    "\n",
    "Choosing the right hyperparameters can have a significant impact on the performance of a machine-learning model. Hyperparameters that are not set properly can lead to overfitting or underfitting, resulting in poor performance of the model. Therefore, selecting the right hyperparameters is an important step in building an accurate and reliable machine-learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723facce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for XGBoost\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "def check(X_train_train, X_train_val, y_train_train, y_train_val):\n",
    "     # Split the training data into training and validation sets\n",
    "#     X_train_train, X_train_val, y_train_train, y_train_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "    model = XGBClassifier(learning_rate=0.1, max_depth=5, n_estimators=100, random_state=42, colsample_bytree=0.8)\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    model.fit(X_train_train, y_train_train)\n",
    "\n",
    "    # Make predictions on the training and validation data\n",
    "    y_train_pred = model.predict(X_train_train)\n",
    "    y_val_pred = model.predict(X_train_val)\n",
    "\n",
    "    # Calculate accuracy on the training and validation data\n",
    "    train_acc = accuracy_score(y_train_train, y_train_pred)\n",
    "    val_acc = accuracy_score(y_train_val, y_val_pred)\n",
    "\n",
    "    # Return the accuracy scores\n",
    "    return train_acc, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf334949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Classifier Start\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_cleaned[['AGE', 'GENDER', 'CHOLESTEROL', 'GLUCOSE', 'SMOKE','ALCOHOL', 'ACTIVE', 'BMI']], df_cleaned['CARDIO_PATIENT'], test_size=0.2, random_state=69)\n",
    "xgb = XGBClassifier(learning_rate=0.1, max_depth=5, n_estimators=100, random_state=42, colsample_bytree=0.8)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the XGBoost model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Define a parameter grid for the XGBoost model\n",
    "# Including hyperparameters to increase the accuracy of the model, and to reduce overfitting\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.2, 0.3, 0.4],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'n_estimators': [50, 100, 200]\n",
    "}\n",
    "\n",
    "# train_acc, val_acc = check(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# print(\"Training Accuracy:\", train_acc)\n",
    "# print(\"Validation Accuracy\", val_acc)\n",
    "\n",
    "# # Create an instance of the GridSearchCV class\n",
    "grid_search = GridSearchCV(xgb, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "\n",
    "# Fit the GridSearchCV instance to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf96963",
   "metadata": {},
   "source": [
    "## 4.2 XGBoost Conclusion: \n",
    "\n",
    "Based on the accuracy score, the model has a accuracy of 64.57% after fine tuning the parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## 4.3 Logistic Regression:\n",
    "\n",
    "#### What is Logistic Regression?\n",
    "\n",
    "Logistic regression is a transformation of the linear regression model that allows us to probabilistically model binary variables. It is also known as a generalized linear model that uses a logit-link. Logistic regression is great when we want to model binary data, just like we are doing here, when we want class probability predictions or when we want some interpretability of the model trough its coefficients we can quantify the impact of each feature on your model‚Äôs predictions via the odds ratio. On the order hand, Logistic Regression is not that great when our data is not linearly separable.\n",
    "\n",
    "---\n",
    "#### How does it work?\n",
    "Logistic regression works very much like linear regression. Input (x) are combined linearly using weights or coefficients values to predict an output (y). The key difference is that the output is modeled as binary values through the equation below:\n",
    "\n",
    "ùë¶ = ùëí‚àó‚àó(ùëè0+ùëè1‚àóùë•)/(1+ùëí‚àó‚àó(ùëè0+ùëè1‚àóùë•))\n",
    " \n",
    "The coefficients (Beta values b) of the logistic regression algorithm are estimated by maximum-likelihood estimation.\n",
    "\n",
    "Maximum-likelihood estimation is a common learning algorithm used by a variety of machine learning algorithms. The best coefficients would result in a model that would predict a value very close to 1 (e.g. male) for the default class and value very close to 0 (e.g. female) for the other class.\n",
    "\n",
    "---\n",
    "#### Advantages of using Logistic Regression:\n",
    "1. Very Simple and easy to understand.\n",
    "2. Interpretable.\n",
    "3. Probabilistically outputs.\n",
    "4. Low cost of maintenance\n",
    "\n",
    "---\n",
    "#### Disadvantages of using Logistic Regression:\n",
    "1. Sensible to highly correlated inputs.\n",
    "2. Assumes Gaussian Distribution.\n",
    "\n",
    "---\n",
    "To make the approach work, you need to pre-process all the features once you create dummy variables, which includes:\n",
    "\n",
    "#### Centering and scaling\n",
    "1. Transformation to remove skewness in data\n",
    "2. Remove highly correlated features\n",
    "3. Remove features that have near-zero variance\n",
    "4. To prevent overfitting, you can use penalized logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88cdadb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Logistic Regression Code:\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalizing Numerical Data\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "# Fit the scaler to the data\n",
    "scaler.fit(df_cleaned[['AGE', 'BMI']])\n",
    "# Transform the data\n",
    "df_cleaned[['AGE', 'BMI']] = scaler.transform(df_cleaned[['AGE', 'BMI']])\n",
    "\n",
    "# Training Model\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_cleaned[['AGE','GENDER', 'CHOLESTEROL', 'GLUCOSE', 'SMOKE','ALCOHOL', 'ACTIVE','BMI']], df_cleaned['CARDIO_PATIENT'], test_size=0.2, random_state=69)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n",
    "print(sb.heatmap(confusion_matrix(y_test, predictions),annot=True,fmt=\".0f\",annot_kws={\"size\":18}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5728a1ae",
   "metadata": {},
   "source": [
    "### What do each of the Performance Metrics mean?\n",
    "\n",
    "Performance metrics such as **Precision, Recall, F1 score, and Support** are commonly used to evaluate the performance of classification models. Here's what each of these metrics means:\n",
    "\n",
    "> **1. Precision:** Precision is the ratio of true positive predictions to the total number of positive predictions made by the model. In other words, it measures how many of the predicted positive instances are actually positive. A high precision score means that the model has a low false positive rate and is good at predicting true positive instances.\n",
    "\n",
    "> **2. Recall:** Recall is the ratio of true positive predictions to the total number of actual positive instances in the dataset. In other words, it measures how many of the actual positive instances are correctly identified by the model. A high recall score means that the model has a low false negative rate and is good at identifying all positive instances.\n",
    "\n",
    "> **3. F1 score:** The F1 score is the harmonic mean of precision and recall. It provides a single score that balances precision and recall. A high F1 score means that the model has both high precision and high recall.\n",
    "\n",
    "> **4. Support:** The support is the number of samples in each class. It gives an indication of how many samples in the dataset belong to each class.\n",
    "\n",
    "When evaluating a model, it is important to consider all of these metrics together, as they can provide a more complete picture of the model's performance. For example, a model with high precision but low recall may be good at identifying positive instances, but may miss many true positive instances. On the other hand, a model with high recall but low precision may identify many positive instances, but may also generate many false positive instances. The F1 score provides a balanced view of both precision and recall, and can help you determine the overall performance of the model.\n",
    "\n",
    "\n",
    "## 4.4 Logistic Regression Conclusion:\n",
    "\n",
    "Based on the accuracy score, the model has a accuracy of 64%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd308554",
   "metadata": {},
   "source": [
    "## 4.4 Neural Network Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ae5c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_cleaned[['AGE', 'GENDER', 'CHOLESTEROL', 'GLUCOSE', 'SMOKE','ALCOHOL', 'ACTIVE', 'BMI']], df_cleaned['CARDIO_PATIENT'], test_size=0.2, random_state=69)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# define parameter grid\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(10,), (50,), (100,)],\n",
    "    'activation': ['logistic', 'tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "    'batch_size': [32, 64, 128],\n",
    "}\n",
    "\n",
    "# create MLPClassifier model\n",
    "model = MLPClassifier(hidden_layer_sizes=(10,10), max_iter=1000)\n",
    "\n",
    "# create GridSearchCV object\n",
    "grid_search = GridSearchCV(model, param_grid=param_grid, cv=5, verbose=1, n_jobs=-1)\n",
    "\n",
    "# fit model with GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print best hyperparameters\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "\n",
    "# clf = MLPClassifier(hidden_layer_sizes=(10,10), max_iter=1000)\n",
    "# clf.fit(X_train, y_train)\n",
    "# y_pred = clf.predict(X_test)\n",
    "# accuracy = clf.score(X_test, y_test)\n",
    "# print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274dce33",
   "metadata": {},
   "source": [
    "---\n",
    "# üñäÔ∏è 5.0 Conclusion:\n",
    "\n",
    "## 5.1 Comparing XGBoost and Logistic Regression:\n",
    "\n",
    "XGBoost Model Accuracy: 64.53%\n",
    "\n",
    "Logistic Regression Accuracy: 64%\n",
    "\n",
    "\n",
    "---\n",
    "## 5.2 Epilogue and Conclusion:\n",
    "\n",
    "Therefore, comparing the 2 models, as the accuracy of the XGBoost Machine Learning Model is higher, it is the better model.\n",
    "\n",
    "---\n",
    "## 5.3 Models Used:\n",
    "1. XGBoost\n",
    "2. Logistic Regression\n",
    "\n",
    "\n",
    "---\n",
    "## 5.4 What did we learn from this project?\n",
    "- Handling imbalanced datasets using resampling methods and imblearn package\n",
    "- Logistic Regression from sklearn\n",
    "- XGBoost from xgboost\n",
    "- Other packages such as tqdm, json, XGBoost\n",
    "- Collaborating using GitHub\n",
    "- Concepts about Precision, Recall, F1 Score and Support\n",
    "\n",
    "---\n",
    "## 5.5 References\n",
    "https://www.nickmccullum.com/python-machine-learning/logistic-regression-python/\n",
    "https://towardsdatascience.com/quick-and-easy-explanation-of-logistics-regression-709df5cc3f1e\n",
    "https://www.digitalocean.com/community/tutorials/normalize-data-in-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
